
#### ########### Nonlinear Regression ###### #######

#### Polynomial Regression ######
#Most code is from Lab 7.8 in ISLR.
library(ISLR) 
attach(Wage) # dont need $ when recall a variable in the model
dim(Wage)

names
range(age)

##Fit a Polynomial Regression Model
#poly( , 3) returns an n-by-3 matrix: each column has mean zero and sample variance 1, and they are orthogonal to each other. 
#The 1st column is a linear combination of age and intercept, the 2nd column is a linear combination of age^2, age, and intercept, 
#and the 3rd column is a linear combination of age^3, age^2, age, and intercept.
tmp = poly(age, 3)
dim(tmp)

colMeans(tmp) #mean zero
round(t(tmp) %*% tmp, dig=4)

fit = lm(wage ~ poly(age, 3), data = Wage)
round(summary(fit)$coef, dig = 3)

#Alternatively we can use the default design matrix where the j-th column corresponds to age^j.
fit2 = lm(wage ~ age + I(age^2) + I(age^3), data=Wage)
round(summary(fit2)$coef, dig = 3)
# The default design matrix can also be generated by poly
# with option raw = TRUE. 
# fit3 should return the same set of coefficients as fit2
# fit3 = lm(wage ~ poly(age, 3, raw = T), data = Wage)
# round(summary(fit2)$coef, dig = 3)

#Note that although the coefficients from fit and the ones from fit2 are different, the t-value and p-value for the last predictor are always the same.
#so if only care about the coefficient of highest order, then doesnt matter which model to choose

#Different ways to fit a polynomial regression model in R. The coefficients might not be the same but the fitted curves are the same.

##Predict the wage at age = 82. fit and fit2 should give you the same answer.
predict(fit, newdata = list(age=82))
predict(fit2, newdata = list(age=82))

##The fitted curve from the all three models should be the same.
agelims = range(age)
age.grid = seq(from = agelims[1], to = agelims[2])
preds = predict(fit, newdata = list(age = age.grid), se=TRUE)
plot(age, wage, xlim = agelims, pch = '.', cex = 2, col="darkgrey")
title("Degree -3 Polynomial ")
lines(age.grid, preds$fit, lwd=2, col="blue")


##Forward Selection on d
#Forward selection for the polynomial order d based on the significance of 
#the coefficient of the highest order starting with quardratic polynomial function, and we finally pick d=3.
summary(lm(wage ~ poly(age, 2), data=Wage))$coef
summary(lm(wage ~ poly(age, 3), data=Wage))$coef
summary(lm(wage ~ poly(age, 4), data=Wage))$coef#highest order insignificant, stop here and pick d=3

##Backward Selection on d
#Back selection for the polynomial order d based on the significance of 
#the coefficient of the highest order starting with d=6, and we finally pick d=3. For this data, 
#the forward and the backward approaches happen to pick the same d value, but in general, the two choices (backward or forward) for d could differ.
summary(lm(wage ~ poly(age, 6), data=Wage))$coef
summary(lm(wage ~ poly(age, 5), data=Wage))$coef
summary(lm(wage ~ poly(age, 4), data=Wage))$coef
summary(lm(wage ~ poly(age, 3), data=Wage))$coef

#* select d and includes all lower order terms by default because when use ploy, the highest order is the linear combination of lower orders

#### Regression Splines ######
#Load packages and read the help files for the two major commands
library(splines);
library(ggplot2)
help(bs)
help(ns)

##Spline Basis Functions
#Basis functions for cubic splines with 5 knots and df = 9 (m knots, the df=m+4)
# in bs function in R, df=real_df-1
x = (1:199)/100;
n = length(x)
m = 5;
myknots = 2*(1:m)/(m+1)
myknots

X = cbind(1, x, x^2, x^3);
for(i in 1:m){
  tmp = (x-myknots[i])^3;
  tmp[tmp<0] = 0;
  X = cbind(X, tmp);
}
plot(c(0,2), range(X), type="n", xlab="", ylab="")
title("Truncated Power Basis")

for(i in 1:(m+4)){
  tmp = X[,i];
  if (i<=4) mylty=1 else mylty=2;
  lines(x[tmp!=0], tmp[tmp!=0], col=i, lty=mylty, lwd=2)
}
for(i in 1:m){
  points(myknots[i], 0, pty="m", pch=19, cex=2)
}

F = bs(x,knots = myknots, intercept = TRUE) # 5+4 m=df-2
dim(F)

mydf = m+4; 
tmpdata = data.frame(t = rep(1:n, mydf),
                     basisfunc=as.vector(F), 
                     type=as.factor(rep(1:mydf, each=n)))
ggplot(tmpdata, aes(x=t, y=basisfunc, color=type)) + 
  geom_path()

mydf = m+4; 
tmpdata = data.frame(t = rep(1:n, mydf),
                     basisfunc=as.vector(F), 
                     type=as.factor(rep(1:mydf, each=n)))
ggplot(tmpdata, aes(x=t, y=basisfunc, color=type)) + 
  geom_path()

#If we do not set intercept = TRUE, then bs will return 9-1 = 8 columns.

F = bs(x, knots = myknots)
dim(F)
mydf = m+3; 
tmpdata = data.frame(t = rep(1:n, mydf),
                     basisfunc=as.vector(F), 
                     type=as.factor(rep(1:mydf, each=n)))
ggplot(tmpdata, aes(x=t, y=basisfunc, color=type)) +
  geom_path()

###Basis functions for NCS with 7 knots (5 interior knots and 2 boundary knots) and df = 7
F = ns(x, knots=myknots, Boundary.knots=c(0,2), intercept=TRUE)
dim(F)
#if intercept = TRUE,thenweneedm=df−2 knots, otherwise we need m = df − 1 knots.

mydf = 7
tmpdata = data.frame(t = rep(1:n, mydf),
                     basisfunc=as.vector(F), 
                     type=as.factor(rep(1:mydf, each=n)))
ggplot(tmpdata, aes(x=t, y=basisfunc, color=type)) +
  geom_path()

### Example: The Birthrates Data #####
#This dataset lists the number of live births per 10,000 23-year-old women in the United States between 1917 and 2003.

source("birthrates.txt");
birthrates = as.data.frame(birthrates)
names(birthrates) = c("year", "rate")
ggplot(birthrates, aes(x=year, y=rate)) + 
  geom_point() + geom_smooth(method="lm", se=FALSE)

#Understand how R counts the df.
#fit1=fit2=fit3
fit1 = lm(rate~bs(year, knots=quantile(year, c(1/3, 2/3))),
          data=birthrates);
fit2 = lm(rate~bs(year, df=5), data=birthrates);
fit3 = lm(rate~bs(year, df=6, intercept=TRUE), data=birthrates) 
fit4 = lm(rate~bs(year, df=5, intercept=TRUE), data=birthrates)

plot(birthrates$year, birthrates$rate, ylim=c(90,280))
lines(spline(birthrates$year, predict(fit1)), col="red", lty=1)
lines(spline(birthrates$year, predict(fit2)), col="blue", lty=2)
lines(spline(birthrates$year, predict(fit3)), col="green", lty=3)
lines(spline(birthrates$year, predict(fit4)), lty=2, lwd=2)


# Alternatively, you can predict the spline fit on a fine grid, and then connect them

# plot(birthrates$year, birthrates$rate, ylim=c(90,280))
# year.grid = seq(from=min(birthrates$year), to=max(birthrates$year), length=200)
# ypred = predict(fit1, data.frame(year=year.grid))
# lines(year.grid, ypred, col="blue", lwd=2)

fit1=lm(rate~ns(year, knots=quantile(year, (1:4)/5)), data=birthrates);
fit2=lm(rate~ns(year, df=5), data=birthrates);
fit3=lm(rate~ns(year, df=6, intercept=TRUE), data=birthrates) 

plot(birthrates$year, birthrates$rate, ylim=c(90,280))
lines(spline(birthrates$year, predict(fit1)), col="red", lty=1)
lines(spline(birthrates$year, predict(fit2)), col="blue", lty=2)
lines(spline(birthrates$year, predict(fit3)), col="green", lty=3)


###Try cubic splines with different degree-of-freedoms
plot(birthrates$year, birthrates$rate, ylim=c(90,280));
lines(spline(birthrates$year, predict(lm(rate~bs(year, df=7), data=birthrates))), col="blue");
lines(spline(birthrates$year, predict(lm(rate~bs(year, df=14), data=birthrates))), col="red");
lines(spline(birthrates$year, predict(lm(rate~bs(year, df=19), data=birthrates))), col="black");
legend("topright", lty=rep(1,3), col=c("blue", "red", "black"), legend=c("df=8", "df=15", "df=20"))

##Make prediction outside the data range
#cubic knots
new = data.frame(year=1905:2015);
fit1=lm(rate~bs(year, df=7), data=birthrates);
pred1=predict(fit1, new);
#natural cubit knots
fit2=lm(rate~ns(year, df=7), data=birthrates);
pred2=predict(fit2, new);
plot(birthrates$year,birthrates$rate, xlim=c(1905,2015),
     ylim=c(min(pred1,pred2), max(pred1,pred2)), 
     ylab="Birth Rate", xlab="Year") 
lines(new$year, pred1, col="red")
lines(new$year, pred2, col="blue")
legend("bottomleft", lty=rep(1,2),  col=c("red",  "blue" ), legend=c("CS with df=7", "NCS with df=7"))

###Use 10-fold CV to select df (or equivalently the number of knots)
#The location of knots will affect the performance of a spline model. But selecting the location of knots is computationally too expensive. Instead, we place knots equally at quantiles of x, and then select just the number of knots, or equivalently, the df. Can we use F-test to select the number of knots?

#For each df, we use 10-fold CV to calculate the CV error. When doing 10-fold CV, each time, based on 90% of the data, we place the (df-4) knots at the corresponding quantiles and then fit a regression spline,

#First, we need to divide the data into K folds.

K=10
n = nrow(birthrates)
fold.size = c(rep(9, 7), rep(8, 3))
fold.id = rep(1:K, fold.size)
fold.id

fold.id = fold.id[sample(1:n, n)]
fold.id

mydf = 10:30
mycv = rep(0, length(mydf))

for(i in 1:length(mydf)){
  m = mydf[i]-4;  
  for(k in 1:K){
    id = which(fold.id == k);
    myknots = quantile(birthrates$year[-id], (1:m)/(m+1))
    myfit = lm(rate ~ bs(year, knots=myknots),
               data=birthrates[-id,])
    ypred = predict(myfit, newdata=birthrates[id,])
    mycv[i]=mycv[i] + sum((birthrates$rate[id] - ypred)^2)
  }
}
plot(mydf, mycv)

#Re-run the 10-fold CV. The plot of mydf versus mycv may vary, but shouldn’t be too different.
fold.id = rep(1:K, fold.size)
fold.id = fold.id[sample(1:n, n)]

mydf = 10:30
mycv = rep(0, length(mydf))

for(i in 1:length(mydf)){
  m = mydf[i]-4;  
  for(k in 1:K){
    id = which(fold.id == k);
    myknots = quantile(birthrates$year[-id], (1:m)/(m+1))
    myfit = lm(rate ~ bs(year, knots=myknots),
               data=birthrates[-id,])
    ypred = predict(myfit, newdata=birthrates[id,])
    mycv[i]=mycv[i] + sum((birthrates$rate[id] - ypred)^2)
  }
}
plot(mydf, mycv)


################ Smoothing Splines ##############
#Load packages and read the help file.

library(splines)
help(smooth.spline)
options(digits = 4)

#A Simulated Example (ESL, chap 5.5.2)
#(x_i,y_i): data points (i=1:30)
#(fx_j, fy_j): true function evaluated on a fine grid (j=1:50)
set.seed(1234)
n = 30 
err = 1
x = sort(runif(n))
y = sin(12*(x+0.2))/(x+0.2) + rnorm(n, 0, err);
plot(x, y, col="red");

fx = 1:50/50;
fy = sin(12*(fx+0.2))/(fx+0.2)
lines(fx, fy, col=8, lwd=2);

#####Fit a smoothing spline model
#Fit smoothing spline models with various dfs.
par(mfrow=c(2,2));      # 2x2 (totally 4) subplots
plot(x,y, xlab='x', ylab='y');
lines(fx, fy, col=8, lwd=1.5);
lines(predict(smooth.spline(x, y, df=5),fx),  lty=2, col='blue', lwd=1.5);
title('df=5');

plot(x,y,xlab='x', ylab='y');
lines(fx, fy, col=8, lwd=1.5);
lines(predict(smooth.spline(x, y, df=9),fx),  lty=2, col='blue', lwd=1.5);
title('df=9');

plot(x,y,xlab='x', ylab='y');
lines(fx, fy, col=8, lwd=1.5);
lines(predict(smooth.spline(x, y, df=15),fx),  lty=2, col='blue', lwd=1.5);
title('df=15');

plot(x,y,xlab='x', ylab='y');
lines(fx, fy, col=8, lwd=1.5);
lines(predict(smooth.spline(x, y, df=20),fx),  lty=2, col='blue', lwd=1.5);
title('df=20')

###Demmler & Reinsch Basis
#Here is how we obtain the DR basis: we first obtain the smoother matrix S (which is not returned y R, so we write our own script to compute it), and then the eigen-vectors of S are basically the DR basis functions.